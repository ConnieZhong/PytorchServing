// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: Tensor.proto

#ifndef PROTOBUF_INCLUDED_Tensor_2eproto
#define PROTOBUF_INCLUDED_Tensor_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_Tensor_2eproto 

namespace protobuf_Tensor_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[2];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_Tensor_2eproto
namespace pytorchserving {
class Tensor;
class TensorDefaultTypeInternal;
extern TensorDefaultTypeInternal _Tensor_default_instance_;
class TensorShape;
class TensorShapeDefaultTypeInternal;
extern TensorShapeDefaultTypeInternal _TensorShape_default_instance_;
}  // namespace pytorchserving
namespace google {
namespace protobuf {
template<> ::pytorchserving::Tensor* Arena::CreateMaybeMessage<::pytorchserving::Tensor>(Arena*);
template<> ::pytorchserving::TensorShape* Arena::CreateMaybeMessage<::pytorchserving::TensorShape>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace pytorchserving {

enum ErrorCode {
  EC_SUCCESS = 0,
  EC_TYPE_ERROR = 1,
  EC_PARM_ERROR = 2,
  EC_MEM_ERROR = 3
};
bool ErrorCode_IsValid(int value);
const ErrorCode ErrorCode_MIN = EC_SUCCESS;
const ErrorCode ErrorCode_MAX = EC_MEM_ERROR;
const int ErrorCode_ARRAYSIZE = ErrorCode_MAX + 1;

const ::google::protobuf::EnumDescriptor* ErrorCode_descriptor();
inline const ::std::string& ErrorCode_Name(ErrorCode value) {
  return ::google::protobuf::internal::NameOfEnum(
    ErrorCode_descriptor(), value);
}
inline bool ErrorCode_Parse(
    const ::std::string& name, ErrorCode* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ErrorCode>(
    ErrorCode_descriptor(), name, value);
}
enum DataType {
  DT_INVALID = 0,
  DT_FLOAT = 1,
  DT_DOUBLE = 2,
  DT_INT32 = 3,
  DT_INT64 = 4,
  DT_BOOL = 5
};
bool DataType_IsValid(int value);
const DataType DataType_MIN = DT_INVALID;
const DataType DataType_MAX = DT_BOOL;
const int DataType_ARRAYSIZE = DataType_MAX + 1;

const ::google::protobuf::EnumDescriptor* DataType_descriptor();
inline const ::std::string& DataType_Name(DataType value) {
  return ::google::protobuf::internal::NameOfEnum(
    DataType_descriptor(), value);
}
inline bool DataType_Parse(
    const ::std::string& name, DataType* value) {
  return ::google::protobuf::internal::ParseNamedEnum<DataType>(
    DataType_descriptor(), name, value);
}
// ===================================================================

class TensorShape : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:pytorchserving.TensorShape) */ {
 public:
  TensorShape();
  virtual ~TensorShape();

  TensorShape(const TensorShape& from);

  inline TensorShape& operator=(const TensorShape& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  TensorShape(TensorShape&& from) noexcept
    : TensorShape() {
    *this = ::std::move(from);
  }

  inline TensorShape& operator=(TensorShape&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const TensorShape& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const TensorShape* internal_default_instance() {
    return reinterpret_cast<const TensorShape*>(
               &_TensorShape_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(TensorShape* other);
  friend void swap(TensorShape& a, TensorShape& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline TensorShape* New() const final {
    return CreateMaybeMessage<TensorShape>(NULL);
  }

  TensorShape* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<TensorShape>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const TensorShape& from);
  void MergeFrom(const TensorShape& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(TensorShape* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int32 dim = 1;
  int dim_size() const;
  void clear_dim();
  static const int kDimFieldNumber = 1;
  ::google::protobuf::int32 dim(int index) const;
  void set_dim(int index, ::google::protobuf::int32 value);
  void add_dim(::google::protobuf::int32 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
      dim() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
      mutable_dim();

  // @@protoc_insertion_point(class_scope:pytorchserving.TensorShape)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 > dim_;
  friend struct ::protobuf_Tensor_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class Tensor : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:pytorchserving.Tensor) */ {
 public:
  Tensor();
  virtual ~Tensor();

  Tensor(const Tensor& from);

  inline Tensor& operator=(const Tensor& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  Tensor(Tensor&& from) noexcept
    : Tensor() {
    *this = ::std::move(from);
  }

  inline Tensor& operator=(Tensor&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const Tensor& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const Tensor* internal_default_instance() {
    return reinterpret_cast<const Tensor*>(
               &_Tensor_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(Tensor* other);
  friend void swap(Tensor& a, Tensor& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline Tensor* New() const final {
    return CreateMaybeMessage<Tensor>(NULL);
  }

  Tensor* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<Tensor>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const Tensor& from);
  void MergeFrom(const Tensor& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(Tensor* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required bytes data = 3;
  bool has_data() const;
  void clear_data();
  static const int kDataFieldNumber = 3;
  const ::std::string& data() const;
  void set_data(const ::std::string& value);
  #if LANG_CXX11
  void set_data(::std::string&& value);
  #endif
  void set_data(const char* value);
  void set_data(const void* value, size_t size);
  ::std::string* mutable_data();
  ::std::string* release_data();
  void set_allocated_data(::std::string* data);

  // required .pytorchserving.TensorShape tensor_shape = 2;
  bool has_tensor_shape() const;
  void clear_tensor_shape();
  static const int kTensorShapeFieldNumber = 2;
  private:
  const ::pytorchserving::TensorShape& _internal_tensor_shape() const;
  public:
  const ::pytorchserving::TensorShape& tensor_shape() const;
  ::pytorchserving::TensorShape* release_tensor_shape();
  ::pytorchserving::TensorShape* mutable_tensor_shape();
  void set_allocated_tensor_shape(::pytorchserving::TensorShape* tensor_shape);

  // required .pytorchserving.DataType dtype = 1;
  bool has_dtype() const;
  void clear_dtype();
  static const int kDtypeFieldNumber = 1;
  ::pytorchserving::DataType dtype() const;
  void set_dtype(::pytorchserving::DataType value);

  // @@protoc_insertion_point(class_scope:pytorchserving.Tensor)
 private:
  void set_has_dtype();
  void clear_has_dtype();
  void set_has_tensor_shape();
  void clear_has_tensor_shape();
  void set_has_data();
  void clear_has_data();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr data_;
  ::pytorchserving::TensorShape* tensor_shape_;
  int dtype_;
  friend struct ::protobuf_Tensor_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// TensorShape

// repeated int32 dim = 1;
inline int TensorShape::dim_size() const {
  return dim_.size();
}
inline void TensorShape::clear_dim() {
  dim_.Clear();
}
inline ::google::protobuf::int32 TensorShape::dim(int index) const {
  // @@protoc_insertion_point(field_get:pytorchserving.TensorShape.dim)
  return dim_.Get(index);
}
inline void TensorShape::set_dim(int index, ::google::protobuf::int32 value) {
  dim_.Set(index, value);
  // @@protoc_insertion_point(field_set:pytorchserving.TensorShape.dim)
}
inline void TensorShape::add_dim(::google::protobuf::int32 value) {
  dim_.Add(value);
  // @@protoc_insertion_point(field_add:pytorchserving.TensorShape.dim)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
TensorShape::dim() const {
  // @@protoc_insertion_point(field_list:pytorchserving.TensorShape.dim)
  return dim_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
TensorShape::mutable_dim() {
  // @@protoc_insertion_point(field_mutable_list:pytorchserving.TensorShape.dim)
  return &dim_;
}

// -------------------------------------------------------------------

// Tensor

// required .pytorchserving.DataType dtype = 1;
inline bool Tensor::has_dtype() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void Tensor::set_has_dtype() {
  _has_bits_[0] |= 0x00000004u;
}
inline void Tensor::clear_has_dtype() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void Tensor::clear_dtype() {
  dtype_ = 0;
  clear_has_dtype();
}
inline ::pytorchserving::DataType Tensor::dtype() const {
  // @@protoc_insertion_point(field_get:pytorchserving.Tensor.dtype)
  return static_cast< ::pytorchserving::DataType >(dtype_);
}
inline void Tensor::set_dtype(::pytorchserving::DataType value) {
  assert(::pytorchserving::DataType_IsValid(value));
  set_has_dtype();
  dtype_ = value;
  // @@protoc_insertion_point(field_set:pytorchserving.Tensor.dtype)
}

// required .pytorchserving.TensorShape tensor_shape = 2;
inline bool Tensor::has_tensor_shape() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void Tensor::set_has_tensor_shape() {
  _has_bits_[0] |= 0x00000002u;
}
inline void Tensor::clear_has_tensor_shape() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void Tensor::clear_tensor_shape() {
  if (tensor_shape_ != NULL) tensor_shape_->Clear();
  clear_has_tensor_shape();
}
inline const ::pytorchserving::TensorShape& Tensor::_internal_tensor_shape() const {
  return *tensor_shape_;
}
inline const ::pytorchserving::TensorShape& Tensor::tensor_shape() const {
  const ::pytorchserving::TensorShape* p = tensor_shape_;
  // @@protoc_insertion_point(field_get:pytorchserving.Tensor.tensor_shape)
  return p != NULL ? *p : *reinterpret_cast<const ::pytorchserving::TensorShape*>(
      &::pytorchserving::_TensorShape_default_instance_);
}
inline ::pytorchserving::TensorShape* Tensor::release_tensor_shape() {
  // @@protoc_insertion_point(field_release:pytorchserving.Tensor.tensor_shape)
  clear_has_tensor_shape();
  ::pytorchserving::TensorShape* temp = tensor_shape_;
  tensor_shape_ = NULL;
  return temp;
}
inline ::pytorchserving::TensorShape* Tensor::mutable_tensor_shape() {
  set_has_tensor_shape();
  if (tensor_shape_ == NULL) {
    auto* p = CreateMaybeMessage<::pytorchserving::TensorShape>(GetArenaNoVirtual());
    tensor_shape_ = p;
  }
  // @@protoc_insertion_point(field_mutable:pytorchserving.Tensor.tensor_shape)
  return tensor_shape_;
}
inline void Tensor::set_allocated_tensor_shape(::pytorchserving::TensorShape* tensor_shape) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete tensor_shape_;
  }
  if (tensor_shape) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      tensor_shape = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, tensor_shape, submessage_arena);
    }
    set_has_tensor_shape();
  } else {
    clear_has_tensor_shape();
  }
  tensor_shape_ = tensor_shape;
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.Tensor.tensor_shape)
}

// required bytes data = 3;
inline bool Tensor::has_data() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void Tensor::set_has_data() {
  _has_bits_[0] |= 0x00000001u;
}
inline void Tensor::clear_has_data() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void Tensor::clear_data() {
  data_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_data();
}
inline const ::std::string& Tensor::data() const {
  // @@protoc_insertion_point(field_get:pytorchserving.Tensor.data)
  return data_.GetNoArena();
}
inline void Tensor::set_data(const ::std::string& value) {
  set_has_data();
  data_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:pytorchserving.Tensor.data)
}
#if LANG_CXX11
inline void Tensor::set_data(::std::string&& value) {
  set_has_data();
  data_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:pytorchserving.Tensor.data)
}
#endif
inline void Tensor::set_data(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_data();
  data_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:pytorchserving.Tensor.data)
}
inline void Tensor::set_data(const void* value, size_t size) {
  set_has_data();
  data_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:pytorchserving.Tensor.data)
}
inline ::std::string* Tensor::mutable_data() {
  set_has_data();
  // @@protoc_insertion_point(field_mutable:pytorchserving.Tensor.data)
  return data_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* Tensor::release_data() {
  // @@protoc_insertion_point(field_release:pytorchserving.Tensor.data)
  if (!has_data()) {
    return NULL;
  }
  clear_has_data();
  return data_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void Tensor::set_allocated_data(::std::string* data) {
  if (data != NULL) {
    set_has_data();
  } else {
    clear_has_data();
  }
  data_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), data);
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.Tensor.data)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace pytorchserving

namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::pytorchserving::ErrorCode> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::pytorchserving::ErrorCode>() {
  return ::pytorchserving::ErrorCode_descriptor();
}
template <> struct is_proto_enum< ::pytorchserving::DataType> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::pytorchserving::DataType>() {
  return ::pytorchserving::DataType_descriptor();
}

}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_Tensor_2eproto
