// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: Predict.proto

#include "Predict.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// This is a temporary google only hack
#ifdef GOOGLE_PROTOBUF_ENFORCE_UNIQUENESS
#include "third_party/protobuf/version.h"
#endif
// @@protoc_insertion_point(includes)

namespace protobuf_Core_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_Core_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_ModelSpec;
}  // namespace protobuf_Core_2eproto
namespace protobuf_Tensor_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_Tensor_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_Tensor;
}  // namespace protobuf_Tensor_2eproto
namespace pytorchserving {
class PredictRequestDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictRequest>
      _instance;
} _PredictRequest_default_instance_;
class PredictResponseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<PredictResponse>
      _instance;
} _PredictResponse_default_instance_;
}  // namespace pytorchserving
namespace protobuf_Predict_2eproto {
static void InitDefaultsPredictRequest() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::pytorchserving::_PredictRequest_default_instance_;
    new (ptr) ::pytorchserving::PredictRequest();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::pytorchserving::PredictRequest::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_PredictRequest =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsPredictRequest}, {
      &protobuf_Core_2eproto::scc_info_ModelSpec.base,
      &protobuf_Tensor_2eproto::scc_info_Tensor.base,}};

static void InitDefaultsPredictResponse() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::pytorchserving::_PredictResponse_default_instance_;
    new (ptr) ::pytorchserving::PredictResponse();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::pytorchserving::PredictResponse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_PredictResponse =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsPredictResponse}, {
      &protobuf_Core_2eproto::scc_info_ModelSpec.base,
      &protobuf_Tensor_2eproto::scc_info_Tensor.base,}};

void InitDefaults() {
  ::google::protobuf::internal::InitSCC(&scc_info_PredictRequest.base);
  ::google::protobuf::internal::InitSCC(&scc_info_PredictResponse.base);
}

::google::protobuf::Metadata file_level_metadata[2];

const ::google::protobuf::uint32 TableStruct::offsets[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictRequest, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictRequest, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictRequest, model_spec_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictRequest, inputs_),
  0,
  1,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, model_spec_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, outputs_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, code_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::pytorchserving::PredictResponse, message_),
  1,
  2,
  3,
  0,
};
static const ::google::protobuf::internal::MigrationSchema schemas[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  { 0, 7, sizeof(::pytorchserving::PredictRequest)},
  { 9, 18, sizeof(::pytorchserving::PredictResponse)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::pytorchserving::_PredictRequest_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::pytorchserving::_PredictResponse_default_instance_),
};

void protobuf_AssignDescriptors() {
  AddDescriptors();
  AssignDescriptors(
      "Predict.proto", schemas, file_default_instances, TableStruct::offsets,
      file_level_metadata, NULL, NULL);
}

void protobuf_AssignDescriptorsOnce() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, protobuf_AssignDescriptors);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_PROTOBUF_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::internal::RegisterAllTypes(file_level_metadata, 2);
}

void AddDescriptorsImpl() {
  InitDefaults();
  static const char descriptor[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
      "\n\rPredict.proto\022\016pytorchserving\032\nCore.pr"
      "oto\032\014Tensor.proto\"g\n\016PredictRequest\022-\n\nm"
      "odel_spec\030\001 \002(\0132\031.pytorchserving.ModelSp"
      "ec\022&\n\006inputs\030\002 \002(\0132\026.pytorchserving.Tens"
      "or\"\210\001\n\017PredictResponse\022-\n\nmodel_spec\030\001 \002"
      "(\0132\031.pytorchserving.ModelSpec\022\'\n\007outputs"
      "\030\002 \002(\0132\026.pytorchserving.Tensor\022\014\n\004code\030\003"
      " \002(\005\022\017\n\007message\030\004 \002(\t2W\n\tPredictor\022J\n\007Pr"
      "edict\022\036.pytorchserving.PredictRequest\032\037."
      "pytorchserving.PredictResponse"
  };
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
      descriptor, 390);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "Predict.proto", &protobuf_RegisterTypes);
  ::protobuf_Core_2eproto::AddDescriptors();
  ::protobuf_Tensor_2eproto::AddDescriptors();
}

void AddDescriptors() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, AddDescriptorsImpl);
}
// Force AddDescriptors() to be called at dynamic initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
}  // namespace protobuf_Predict_2eproto
namespace pytorchserving {

// ===================================================================

void PredictRequest::InitAsDefaultInstance() {
  ::pytorchserving::_PredictRequest_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::pytorchserving::ModelSpec*>(
      ::pytorchserving::ModelSpec::internal_default_instance());
  ::pytorchserving::_PredictRequest_default_instance_._instance.get_mutable()->inputs_ = const_cast< ::pytorchserving::Tensor*>(
      ::pytorchserving::Tensor::internal_default_instance());
}
void PredictRequest::clear_model_spec() {
  if (model_spec_ != NULL) model_spec_->Clear();
  clear_has_model_spec();
}
void PredictRequest::clear_inputs() {
  if (inputs_ != NULL) inputs_->Clear();
  clear_has_inputs();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictRequest::kModelSpecFieldNumber;
const int PredictRequest::kInputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictRequest::PredictRequest()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_Predict_2eproto::scc_info_PredictRequest.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:pytorchserving.PredictRequest)
}
PredictRequest::PredictRequest(const PredictRequest& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_model_spec()) {
    model_spec_ = new ::pytorchserving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = NULL;
  }
  if (from.has_inputs()) {
    inputs_ = new ::pytorchserving::Tensor(*from.inputs_);
  } else {
    inputs_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:pytorchserving.PredictRequest)
}

void PredictRequest::SharedCtor() {
  ::memset(&model_spec_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&inputs_) -
      reinterpret_cast<char*>(&model_spec_)) + sizeof(inputs_));
}

PredictRequest::~PredictRequest() {
  // @@protoc_insertion_point(destructor:pytorchserving.PredictRequest)
  SharedDtor();
}

void PredictRequest::SharedDtor() {
  if (this != internal_default_instance()) delete model_spec_;
  if (this != internal_default_instance()) delete inputs_;
}

void PredictRequest::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* PredictRequest::descriptor() {
  ::protobuf_Predict_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_Predict_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const PredictRequest& PredictRequest::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_Predict_2eproto::scc_info_PredictRequest.base);
  return *internal_default_instance();
}


void PredictRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:pytorchserving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      GOOGLE_DCHECK(model_spec_ != NULL);
      model_spec_->Clear();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(inputs_ != NULL);
      inputs_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool PredictRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:pytorchserving.PredictRequest)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .pytorchserving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .pytorchserving.Tensor inputs = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_inputs()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:pytorchserving.PredictRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:pytorchserving.PredictRequest)
  return false;
#undef DO_
}

void PredictRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:pytorchserving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .pytorchserving.ModelSpec model_spec = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_model_spec(), output);
  }

  // required .pytorchserving.Tensor inputs = 2;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_inputs(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:pytorchserving.PredictRequest)
}

::google::protobuf::uint8* PredictRequest::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:pytorchserving.PredictRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .pytorchserving.ModelSpec model_spec = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_model_spec(), deterministic, target);
  }

  // required .pytorchserving.Tensor inputs = 2;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_inputs(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:pytorchserving.PredictRequest)
  return target;
}

size_t PredictRequest::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:pytorchserving.PredictRequest)
  size_t total_size = 0;

  if (has_model_spec()) {
    // required .pytorchserving.ModelSpec model_spec = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  if (has_inputs()) {
    // required .pytorchserving.Tensor inputs = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *inputs_);
  }

  return total_size;
}
size_t PredictRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:pytorchserving.PredictRequest)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required .pytorchserving.ModelSpec model_spec = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);

    // required .pytorchserving.Tensor inputs = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *inputs_);

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void PredictRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:pytorchserving.PredictRequest)
  GOOGLE_DCHECK_NE(&from, this);
  const PredictRequest* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictRequest>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:pytorchserving.PredictRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:pytorchserving.PredictRequest)
    MergeFrom(*source);
  }
}

void PredictRequest::MergeFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:pytorchserving.PredictRequest)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_model_spec()->::pytorchserving::ModelSpec::MergeFrom(from.model_spec());
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_inputs()->::pytorchserving::Tensor::MergeFrom(from.inputs());
    }
  }
}

void PredictRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:pytorchserving.PredictRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictRequest::CopyFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:pytorchserving.PredictRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictRequest::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  if (has_model_spec()) {
    if (!this->model_spec_->IsInitialized()) return false;
  }
  if (has_inputs()) {
    if (!this->inputs_->IsInitialized()) return false;
  }
  return true;
}

void PredictRequest::Swap(PredictRequest* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PredictRequest::InternalSwap(PredictRequest* other) {
  using std::swap;
  swap(model_spec_, other->model_spec_);
  swap(inputs_, other->inputs_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata PredictRequest::GetMetadata() const {
  protobuf_Predict_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_Predict_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void PredictResponse::InitAsDefaultInstance() {
  ::pytorchserving::_PredictResponse_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::pytorchserving::ModelSpec*>(
      ::pytorchserving::ModelSpec::internal_default_instance());
  ::pytorchserving::_PredictResponse_default_instance_._instance.get_mutable()->outputs_ = const_cast< ::pytorchserving::Tensor*>(
      ::pytorchserving::Tensor::internal_default_instance());
}
void PredictResponse::clear_model_spec() {
  if (model_spec_ != NULL) model_spec_->Clear();
  clear_has_model_spec();
}
void PredictResponse::clear_outputs() {
  if (outputs_ != NULL) outputs_->Clear();
  clear_has_outputs();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictResponse::kModelSpecFieldNumber;
const int PredictResponse::kOutputsFieldNumber;
const int PredictResponse::kCodeFieldNumber;
const int PredictResponse::kMessageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictResponse::PredictResponse()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_Predict_2eproto::scc_info_PredictResponse.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:pytorchserving.PredictResponse)
}
PredictResponse::PredictResponse(const PredictResponse& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  message_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_message()) {
    message_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.message_);
  }
  if (from.has_model_spec()) {
    model_spec_ = new ::pytorchserving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = NULL;
  }
  if (from.has_outputs()) {
    outputs_ = new ::pytorchserving::Tensor(*from.outputs_);
  } else {
    outputs_ = NULL;
  }
  code_ = from.code_;
  // @@protoc_insertion_point(copy_constructor:pytorchserving.PredictResponse)
}

void PredictResponse::SharedCtor() {
  message_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&model_spec_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&code_) -
      reinterpret_cast<char*>(&model_spec_)) + sizeof(code_));
}

PredictResponse::~PredictResponse() {
  // @@protoc_insertion_point(destructor:pytorchserving.PredictResponse)
  SharedDtor();
}

void PredictResponse::SharedDtor() {
  message_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete model_spec_;
  if (this != internal_default_instance()) delete outputs_;
}

void PredictResponse::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* PredictResponse::descriptor() {
  ::protobuf_Predict_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_Predict_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const PredictResponse& PredictResponse::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_Predict_2eproto::scc_info_PredictResponse.base);
  return *internal_default_instance();
}


void PredictResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:pytorchserving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      message_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(model_spec_ != NULL);
      model_spec_->Clear();
    }
    if (cached_has_bits & 0x00000004u) {
      GOOGLE_DCHECK(outputs_ != NULL);
      outputs_->Clear();
    }
  }
  code_ = 0;
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool PredictResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:pytorchserving.PredictResponse)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .pytorchserving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .pytorchserving.Tensor outputs = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_outputs()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required int32 code = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_code();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &code_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string message = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_message()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->message().data(), static_cast<int>(this->message().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "pytorchserving.PredictResponse.message");
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:pytorchserving.PredictResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:pytorchserving.PredictResponse)
  return false;
#undef DO_
}

void PredictResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:pytorchserving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .pytorchserving.ModelSpec model_spec = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_model_spec(), output);
  }

  // required .pytorchserving.Tensor outputs = 2;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_outputs(), output);
  }

  // required int32 code = 3;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(3, this->code(), output);
  }

  // required string message = 4;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->message().data(), static_cast<int>(this->message().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "pytorchserving.PredictResponse.message");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      4, this->message(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:pytorchserving.PredictResponse)
}

::google::protobuf::uint8* PredictResponse::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:pytorchserving.PredictResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .pytorchserving.ModelSpec model_spec = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_model_spec(), deterministic, target);
  }

  // required .pytorchserving.Tensor outputs = 2;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_outputs(), deterministic, target);
  }

  // required int32 code = 3;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteInt32ToArray(3, this->code(), target);
  }

  // required string message = 4;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->message().data(), static_cast<int>(this->message().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "pytorchserving.PredictResponse.message");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        4, this->message(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:pytorchserving.PredictResponse)
  return target;
}

size_t PredictResponse::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:pytorchserving.PredictResponse)
  size_t total_size = 0;

  if (has_message()) {
    // required string message = 4;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->message());
  }

  if (has_model_spec()) {
    // required .pytorchserving.ModelSpec model_spec = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  if (has_outputs()) {
    // required .pytorchserving.Tensor outputs = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *outputs_);
  }

  if (has_code()) {
    // required int32 code = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->code());
  }

  return total_size;
}
size_t PredictResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:pytorchserving.PredictResponse)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x0000000f) ^ 0x0000000f) == 0) {  // All required fields are present.
    // required string message = 4;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->message());

    // required .pytorchserving.ModelSpec model_spec = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);

    // required .pytorchserving.Tensor outputs = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *outputs_);

    // required int32 code = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->code());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void PredictResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:pytorchserving.PredictResponse)
  GOOGLE_DCHECK_NE(&from, this);
  const PredictResponse* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictResponse>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:pytorchserving.PredictResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:pytorchserving.PredictResponse)
    MergeFrom(*source);
  }
}

void PredictResponse::MergeFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:pytorchserving.PredictResponse)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_message();
      message_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.message_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_model_spec()->::pytorchserving::ModelSpec::MergeFrom(from.model_spec());
    }
    if (cached_has_bits & 0x00000004u) {
      mutable_outputs()->::pytorchserving::Tensor::MergeFrom(from.outputs());
    }
    if (cached_has_bits & 0x00000008u) {
      code_ = from.code_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void PredictResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:pytorchserving.PredictResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictResponse::CopyFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:pytorchserving.PredictResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictResponse::IsInitialized() const {
  if ((_has_bits_[0] & 0x0000000f) != 0x0000000f) return false;
  if (has_model_spec()) {
    if (!this->model_spec_->IsInitialized()) return false;
  }
  if (has_outputs()) {
    if (!this->outputs_->IsInitialized()) return false;
  }
  return true;
}

void PredictResponse::Swap(PredictResponse* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PredictResponse::InternalSwap(PredictResponse* other) {
  using std::swap;
  message_.Swap(&other->message_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(model_spec_, other->model_spec_);
  swap(outputs_, other->outputs_);
  swap(code_, other->code_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata PredictResponse::GetMetadata() const {
  protobuf_Predict_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_Predict_2eproto::file_level_metadata[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace pytorchserving
namespace google {
namespace protobuf {
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::pytorchserving::PredictRequest* Arena::CreateMaybeMessage< ::pytorchserving::PredictRequest >(Arena* arena) {
  return Arena::CreateInternal< ::pytorchserving::PredictRequest >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::pytorchserving::PredictResponse* Arena::CreateMaybeMessage< ::pytorchserving::PredictResponse >(Arena* arena) {
  return Arena::CreateInternal< ::pytorchserving::PredictResponse >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
