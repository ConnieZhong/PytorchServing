// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: Predict.proto

#ifndef PROTOBUF_INCLUDED_Predict_2eproto
#define PROTOBUF_INCLUDED_Predict_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "Core.pb.h"
#include "Tensor.pb.h"
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_Predict_2eproto 

namespace protobuf_Predict_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[2];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_Predict_2eproto
namespace pytorchserving {
class PredictRequest;
class PredictRequestDefaultTypeInternal;
extern PredictRequestDefaultTypeInternal _PredictRequest_default_instance_;
class PredictResponse;
class PredictResponseDefaultTypeInternal;
extern PredictResponseDefaultTypeInternal _PredictResponse_default_instance_;
}  // namespace pytorchserving
namespace google {
namespace protobuf {
template<> ::pytorchserving::PredictRequest* Arena::CreateMaybeMessage<::pytorchserving::PredictRequest>(Arena*);
template<> ::pytorchserving::PredictResponse* Arena::CreateMaybeMessage<::pytorchserving::PredictResponse>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace pytorchserving {

// ===================================================================

class PredictRequest : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:pytorchserving.PredictRequest) */ {
 public:
  PredictRequest();
  virtual ~PredictRequest();

  PredictRequest(const PredictRequest& from);

  inline PredictRequest& operator=(const PredictRequest& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  PredictRequest(PredictRequest&& from) noexcept
    : PredictRequest() {
    *this = ::std::move(from);
  }

  inline PredictRequest& operator=(PredictRequest&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const PredictRequest& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const PredictRequest* internal_default_instance() {
    return reinterpret_cast<const PredictRequest*>(
               &_PredictRequest_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(PredictRequest* other);
  friend void swap(PredictRequest& a, PredictRequest& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline PredictRequest* New() const final {
    return CreateMaybeMessage<PredictRequest>(NULL);
  }

  PredictRequest* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<PredictRequest>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const PredictRequest& from);
  void MergeFrom(const PredictRequest& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(PredictRequest* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required .pytorchserving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec();
  static const int kModelSpecFieldNumber = 1;
  private:
  const ::pytorchserving::ModelSpec& _internal_model_spec() const;
  public:
  const ::pytorchserving::ModelSpec& model_spec() const;
  ::pytorchserving::ModelSpec* release_model_spec();
  ::pytorchserving::ModelSpec* mutable_model_spec();
  void set_allocated_model_spec(::pytorchserving::ModelSpec* model_spec);

  // required .pytorchserving.Tensor inputs = 2;
  bool has_inputs() const;
  void clear_inputs();
  static const int kInputsFieldNumber = 2;
  private:
  const ::pytorchserving::Tensor& _internal_inputs() const;
  public:
  const ::pytorchserving::Tensor& inputs() const;
  ::pytorchserving::Tensor* release_inputs();
  ::pytorchserving::Tensor* mutable_inputs();
  void set_allocated_inputs(::pytorchserving::Tensor* inputs);

  // @@protoc_insertion_point(class_scope:pytorchserving.PredictRequest)
 private:
  void set_has_model_spec();
  void clear_has_model_spec();
  void set_has_inputs();
  void clear_has_inputs();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::pytorchserving::ModelSpec* model_spec_;
  ::pytorchserving::Tensor* inputs_;
  friend struct ::protobuf_Predict_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PredictResponse : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:pytorchserving.PredictResponse) */ {
 public:
  PredictResponse();
  virtual ~PredictResponse();

  PredictResponse(const PredictResponse& from);

  inline PredictResponse& operator=(const PredictResponse& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  PredictResponse(PredictResponse&& from) noexcept
    : PredictResponse() {
    *this = ::std::move(from);
  }

  inline PredictResponse& operator=(PredictResponse&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const PredictResponse& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const PredictResponse* internal_default_instance() {
    return reinterpret_cast<const PredictResponse*>(
               &_PredictResponse_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(PredictResponse* other);
  friend void swap(PredictResponse& a, PredictResponse& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline PredictResponse* New() const final {
    return CreateMaybeMessage<PredictResponse>(NULL);
  }

  PredictResponse* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<PredictResponse>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const PredictResponse& from);
  void MergeFrom(const PredictResponse& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(PredictResponse* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required string message = 4;
  bool has_message() const;
  void clear_message();
  static const int kMessageFieldNumber = 4;
  const ::std::string& message() const;
  void set_message(const ::std::string& value);
  #if LANG_CXX11
  void set_message(::std::string&& value);
  #endif
  void set_message(const char* value);
  void set_message(const char* value, size_t size);
  ::std::string* mutable_message();
  ::std::string* release_message();
  void set_allocated_message(::std::string* message);

  // required .pytorchserving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec();
  static const int kModelSpecFieldNumber = 1;
  private:
  const ::pytorchserving::ModelSpec& _internal_model_spec() const;
  public:
  const ::pytorchserving::ModelSpec& model_spec() const;
  ::pytorchserving::ModelSpec* release_model_spec();
  ::pytorchserving::ModelSpec* mutable_model_spec();
  void set_allocated_model_spec(::pytorchserving::ModelSpec* model_spec);

  // required .pytorchserving.Tensor outputs = 2;
  bool has_outputs() const;
  void clear_outputs();
  static const int kOutputsFieldNumber = 2;
  private:
  const ::pytorchserving::Tensor& _internal_outputs() const;
  public:
  const ::pytorchserving::Tensor& outputs() const;
  ::pytorchserving::Tensor* release_outputs();
  ::pytorchserving::Tensor* mutable_outputs();
  void set_allocated_outputs(::pytorchserving::Tensor* outputs);

  // required int32 code = 3;
  bool has_code() const;
  void clear_code();
  static const int kCodeFieldNumber = 3;
  ::google::protobuf::int32 code() const;
  void set_code(::google::protobuf::int32 value);

  // @@protoc_insertion_point(class_scope:pytorchserving.PredictResponse)
 private:
  void set_has_model_spec();
  void clear_has_model_spec();
  void set_has_outputs();
  void clear_has_outputs();
  void set_has_code();
  void clear_has_code();
  void set_has_message();
  void clear_has_message();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr message_;
  ::pytorchserving::ModelSpec* model_spec_;
  ::pytorchserving::Tensor* outputs_;
  ::google::protobuf::int32 code_;
  friend struct ::protobuf_Predict_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// PredictRequest

// required .pytorchserving.ModelSpec model_spec = 1;
inline bool PredictRequest::has_model_spec() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void PredictRequest::set_has_model_spec() {
  _has_bits_[0] |= 0x00000001u;
}
inline void PredictRequest::clear_has_model_spec() {
  _has_bits_[0] &= ~0x00000001u;
}
inline const ::pytorchserving::ModelSpec& PredictRequest::_internal_model_spec() const {
  return *model_spec_;
}
inline const ::pytorchserving::ModelSpec& PredictRequest::model_spec() const {
  const ::pytorchserving::ModelSpec* p = model_spec_;
  // @@protoc_insertion_point(field_get:pytorchserving.PredictRequest.model_spec)
  return p != NULL ? *p : *reinterpret_cast<const ::pytorchserving::ModelSpec*>(
      &::pytorchserving::_ModelSpec_default_instance_);
}
inline ::pytorchserving::ModelSpec* PredictRequest::release_model_spec() {
  // @@protoc_insertion_point(field_release:pytorchserving.PredictRequest.model_spec)
  clear_has_model_spec();
  ::pytorchserving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
inline ::pytorchserving::ModelSpec* PredictRequest::mutable_model_spec() {
  set_has_model_spec();
  if (model_spec_ == NULL) {
    auto* p = CreateMaybeMessage<::pytorchserving::ModelSpec>(GetArenaNoVirtual());
    model_spec_ = p;
  }
  // @@protoc_insertion_point(field_mutable:pytorchserving.PredictRequest.model_spec)
  return model_spec_;
}
inline void PredictRequest::set_allocated_model_spec(::pytorchserving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(model_spec_);
  }
  if (model_spec) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      model_spec = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, model_spec, submessage_arena);
    }
    set_has_model_spec();
  } else {
    clear_has_model_spec();
  }
  model_spec_ = model_spec;
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.PredictRequest.model_spec)
}

// required .pytorchserving.Tensor inputs = 2;
inline bool PredictRequest::has_inputs() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void PredictRequest::set_has_inputs() {
  _has_bits_[0] |= 0x00000002u;
}
inline void PredictRequest::clear_has_inputs() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::pytorchserving::Tensor& PredictRequest::_internal_inputs() const {
  return *inputs_;
}
inline const ::pytorchserving::Tensor& PredictRequest::inputs() const {
  const ::pytorchserving::Tensor* p = inputs_;
  // @@protoc_insertion_point(field_get:pytorchserving.PredictRequest.inputs)
  return p != NULL ? *p : *reinterpret_cast<const ::pytorchserving::Tensor*>(
      &::pytorchserving::_Tensor_default_instance_);
}
inline ::pytorchserving::Tensor* PredictRequest::release_inputs() {
  // @@protoc_insertion_point(field_release:pytorchserving.PredictRequest.inputs)
  clear_has_inputs();
  ::pytorchserving::Tensor* temp = inputs_;
  inputs_ = NULL;
  return temp;
}
inline ::pytorchserving::Tensor* PredictRequest::mutable_inputs() {
  set_has_inputs();
  if (inputs_ == NULL) {
    auto* p = CreateMaybeMessage<::pytorchserving::Tensor>(GetArenaNoVirtual());
    inputs_ = p;
  }
  // @@protoc_insertion_point(field_mutable:pytorchserving.PredictRequest.inputs)
  return inputs_;
}
inline void PredictRequest::set_allocated_inputs(::pytorchserving::Tensor* inputs) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(inputs_);
  }
  if (inputs) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      inputs = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, inputs, submessage_arena);
    }
    set_has_inputs();
  } else {
    clear_has_inputs();
  }
  inputs_ = inputs;
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.PredictRequest.inputs)
}

// -------------------------------------------------------------------

// PredictResponse

// required .pytorchserving.ModelSpec model_spec = 1;
inline bool PredictResponse::has_model_spec() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void PredictResponse::set_has_model_spec() {
  _has_bits_[0] |= 0x00000002u;
}
inline void PredictResponse::clear_has_model_spec() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::pytorchserving::ModelSpec& PredictResponse::_internal_model_spec() const {
  return *model_spec_;
}
inline const ::pytorchserving::ModelSpec& PredictResponse::model_spec() const {
  const ::pytorchserving::ModelSpec* p = model_spec_;
  // @@protoc_insertion_point(field_get:pytorchserving.PredictResponse.model_spec)
  return p != NULL ? *p : *reinterpret_cast<const ::pytorchserving::ModelSpec*>(
      &::pytorchserving::_ModelSpec_default_instance_);
}
inline ::pytorchserving::ModelSpec* PredictResponse::release_model_spec() {
  // @@protoc_insertion_point(field_release:pytorchserving.PredictResponse.model_spec)
  clear_has_model_spec();
  ::pytorchserving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
inline ::pytorchserving::ModelSpec* PredictResponse::mutable_model_spec() {
  set_has_model_spec();
  if (model_spec_ == NULL) {
    auto* p = CreateMaybeMessage<::pytorchserving::ModelSpec>(GetArenaNoVirtual());
    model_spec_ = p;
  }
  // @@protoc_insertion_point(field_mutable:pytorchserving.PredictResponse.model_spec)
  return model_spec_;
}
inline void PredictResponse::set_allocated_model_spec(::pytorchserving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(model_spec_);
  }
  if (model_spec) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      model_spec = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, model_spec, submessage_arena);
    }
    set_has_model_spec();
  } else {
    clear_has_model_spec();
  }
  model_spec_ = model_spec;
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.PredictResponse.model_spec)
}

// required .pytorchserving.Tensor outputs = 2;
inline bool PredictResponse::has_outputs() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void PredictResponse::set_has_outputs() {
  _has_bits_[0] |= 0x00000004u;
}
inline void PredictResponse::clear_has_outputs() {
  _has_bits_[0] &= ~0x00000004u;
}
inline const ::pytorchserving::Tensor& PredictResponse::_internal_outputs() const {
  return *outputs_;
}
inline const ::pytorchserving::Tensor& PredictResponse::outputs() const {
  const ::pytorchserving::Tensor* p = outputs_;
  // @@protoc_insertion_point(field_get:pytorchserving.PredictResponse.outputs)
  return p != NULL ? *p : *reinterpret_cast<const ::pytorchserving::Tensor*>(
      &::pytorchserving::_Tensor_default_instance_);
}
inline ::pytorchserving::Tensor* PredictResponse::release_outputs() {
  // @@protoc_insertion_point(field_release:pytorchserving.PredictResponse.outputs)
  clear_has_outputs();
  ::pytorchserving::Tensor* temp = outputs_;
  outputs_ = NULL;
  return temp;
}
inline ::pytorchserving::Tensor* PredictResponse::mutable_outputs() {
  set_has_outputs();
  if (outputs_ == NULL) {
    auto* p = CreateMaybeMessage<::pytorchserving::Tensor>(GetArenaNoVirtual());
    outputs_ = p;
  }
  // @@protoc_insertion_point(field_mutable:pytorchserving.PredictResponse.outputs)
  return outputs_;
}
inline void PredictResponse::set_allocated_outputs(::pytorchserving::Tensor* outputs) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(outputs_);
  }
  if (outputs) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      outputs = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, outputs, submessage_arena);
    }
    set_has_outputs();
  } else {
    clear_has_outputs();
  }
  outputs_ = outputs;
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.PredictResponse.outputs)
}

// required int32 code = 3;
inline bool PredictResponse::has_code() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void PredictResponse::set_has_code() {
  _has_bits_[0] |= 0x00000008u;
}
inline void PredictResponse::clear_has_code() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void PredictResponse::clear_code() {
  code_ = 0;
  clear_has_code();
}
inline ::google::protobuf::int32 PredictResponse::code() const {
  // @@protoc_insertion_point(field_get:pytorchserving.PredictResponse.code)
  return code_;
}
inline void PredictResponse::set_code(::google::protobuf::int32 value) {
  set_has_code();
  code_ = value;
  // @@protoc_insertion_point(field_set:pytorchserving.PredictResponse.code)
}

// required string message = 4;
inline bool PredictResponse::has_message() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void PredictResponse::set_has_message() {
  _has_bits_[0] |= 0x00000001u;
}
inline void PredictResponse::clear_has_message() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void PredictResponse::clear_message() {
  message_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_message();
}
inline const ::std::string& PredictResponse::message() const {
  // @@protoc_insertion_point(field_get:pytorchserving.PredictResponse.message)
  return message_.GetNoArena();
}
inline void PredictResponse::set_message(const ::std::string& value) {
  set_has_message();
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:pytorchserving.PredictResponse.message)
}
#if LANG_CXX11
inline void PredictResponse::set_message(::std::string&& value) {
  set_has_message();
  message_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:pytorchserving.PredictResponse.message)
}
#endif
inline void PredictResponse::set_message(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_message();
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:pytorchserving.PredictResponse.message)
}
inline void PredictResponse::set_message(const char* value, size_t size) {
  set_has_message();
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:pytorchserving.PredictResponse.message)
}
inline ::std::string* PredictResponse::mutable_message() {
  set_has_message();
  // @@protoc_insertion_point(field_mutable:pytorchserving.PredictResponse.message)
  return message_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* PredictResponse::release_message() {
  // @@protoc_insertion_point(field_release:pytorchserving.PredictResponse.message)
  if (!has_message()) {
    return NULL;
  }
  clear_has_message();
  return message_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void PredictResponse::set_allocated_message(::std::string* message) {
  if (message != NULL) {
    set_has_message();
  } else {
    clear_has_message();
  }
  message_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), message);
  // @@protoc_insertion_point(field_set_allocated:pytorchserving.PredictResponse.message)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace pytorchserving

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_Predict_2eproto
